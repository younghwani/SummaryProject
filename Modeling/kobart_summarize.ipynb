{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "kobart_summarize.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNoH9u+S4vu/SoSbz0gsAni"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "smftlveiijfj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d391ccc3-f7ce-4098-820f-4a52e021084a"
      },
      "source": [
        "# gpu 사용 체크\n",
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sat Nov 20 04:45:22 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 495.44       Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   34C    P0    26W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3ow4JKQEWwKb",
        "outputId": "b7acdb03-21a4-4dfc-c486-91c017618aef"
      },
      "source": [
        "# 최초 1회만 실행하면 된다.\n",
        "# git clone\n",
        "# !git clone https://github.com/seujung/KoBART-summarization.git"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'KoBART-summarization'...\n",
            "remote: Enumerating objects: 134, done.\u001b[K\n",
            "remote: Counting objects: 100% (134/134), done.\u001b[K\n",
            "remote: Compressing objects: 100% (98/98), done.\u001b[K\n",
            "remote: Total 134 (delta 71), reused 85 (delta 33), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (134/134), 37.23 MiB | 18.36 MiB/s, done.\n",
            "Resolving deltas: 100% (71/71), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E5Ncr4UBqqni",
        "outputId": "cf447696-d1ae-4aa8-b1a4-52c4281fce19"
      },
      "source": [
        "# 드라이브 연결\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "Xu9o45o2qfNT",
        "outputId": "8cd9e8e0-47a6-4029-9160-ea8466b286b4"
      },
      "source": [
        "# 클론한 파일 이동\n",
        "# 최초 1회만 실행하면 된다.\n",
        "# import shutil\n",
        "\n",
        "# src = \"/content/KoBART-summarization\"\n",
        "# dst = \"/content/drive/MyDrive/\"\n",
        "\n",
        "# shutil.move(src, dst)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content/drive/MyDrive/KoBART-summarization'"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f4-sQtJMXMMI"
      },
      "source": [
        "# 필요 패키지 설치\n",
        "%cd /content/drive/MyDrive/KoBART-summarization\n",
        "!pip install -r requirements.txt\n",
        "# !pip install git+https://github.com/SKT-AI/KoBART#egg=kobart\n",
        "!pip install torch==1.7.1+cu101 torchvision==0.8.2+cu101 -f https://download.pytorch.org/whl/torch_stable.html"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "avsxf9O_XMGW",
        "outputId": "fabe3e8b-75c0-4d85-b77c-ed0a9317311a"
      },
      "source": [
        "# 클론한 파일 제공 데이터\n",
        "%cd data\n",
        "# !tar -zxvf train.tar.gz\n",
        "# !tar -zxvf test.tar.gz\n",
        "!ls"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/My Drive/KoBART-summarization/data\n",
            "train.tsv\n",
            "test.tsv\n",
            "test.tar.gz  test.tsv  train.tar.gz  train.tsv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zJo2h3qqs3Ap"
      },
      "source": [
        "✅  사용할 디렉토리/data/ 아래에 사용할 train.tsv, test.tsv 파일로 변경한다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_HdFg7SjXo0M"
      },
      "source": [
        "# 파일 확인\n",
        "# !cat train.tsv\n",
        "# !cat test.tsv"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dD8M_WAFXeHr",
        "outputId": "609fdf21-29ba-410e-86f9-c2655c4acb6d"
      },
      "source": [
        "%cd ../\n",
        "!pwd\n",
        "!ls"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/My Drive/KoBART-summarization\n",
            "/content/drive/My Drive/KoBART-summarization\n",
            "data\t\t     imgs\t\tlogs\t\t  rouge_metric.py\n",
            "dataset.py\t     infer.py\t\t__pycache__\t  run_train_cpu.sh\n",
            "download_binary.py   install_kobart.sh\tREADME.md\t  run_train.sh\n",
            "get_model_binary.py  LICENSE\t\trequirements.txt  train.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qRXbWLepzKVr"
      },
      "source": [
        "!pip install torchtext==0.8.0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sNBz6lwO4jD0"
      },
      "source": [
        "# check file & tokenizing\n",
        "\n",
        "# import os\n",
        "# import glob\n",
        "# import torch\n",
        "# import ast\n",
        "# import numpy as np\n",
        "# import pandas as pd\n",
        "# from tqdm import tqdm, trange\n",
        "# from torch.utils.data import Dataset, DataLoader, IterableDataset\n",
        "\n",
        "# from transformers import BartForConditionalGeneration, PreTrainedTokenizerFast\n",
        "\n",
        "# docs = pd.read_csv(\"/content/drive/MyDrive/KoBART-summarization/data/test.tsv\", sep='\\t')\n",
        "# instance = docs.iloc[2]\n",
        "\n",
        "# instance['news']\n",
        "# summ = instance['summary']\n",
        "\n",
        "# tok = PreTrainedTokenizerFast.from_pretrained('gogamza/kobart-base-v2')\n",
        "\n",
        "# id = tok.encode(summ)\n",
        "# id"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9igkB8J9XBbX",
        "outputId": "84a15f9a-4c27-4c68-e902-c41feaeb05b1"
      },
      "source": [
        "# gpu\n",
        "!python train.py --gradient_clip_val 1.0 --max_epochs 2 --default_root_dir logs --gpus 1 --batch_size 10 --num_workers 4\n",
        "\n",
        "# cpu\n",
        "# !python train.py  --gradient_clip_val 1.0 --max_epochs 50 --default_root_dir logs  --batch_size 4 --num_workers 4\n",
        "\n",
        "# shell\n",
        "# !sh run_train.sh\n",
        "# !sh run_train_cpu.sh"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:root:Namespace(accelerator=None, accumulate_grad_batches=1, amp_backend='native', amp_level='O2', auto_lr_find=False, auto_scale_batch_size=False, auto_select_gpus=False, batch_size=10, benchmark=False, check_val_every_n_epoch=1, checkpoint_callback=True, checkpoint_path=None, default_root_dir='logs', deterministic=False, distributed_backend=None, fast_dev_run=False, flush_logs_every_n_steps=100, gpus=1, gradient_clip_algorithm='norm', gradient_clip_val=1.0, limit_predict_batches=1.0, limit_test_batches=1.0, limit_train_batches=1.0, limit_val_batches=1.0, log_every_n_steps=50, log_gpu_memory=None, logger=True, lr=3e-05, max_epochs=2, max_len=512, max_steps=None, max_time=None, min_epochs=None, min_steps=None, model_path=None, move_metrics_to_cpu=False, multiple_trainloader_mode='max_size_cycle', num_nodes=1, num_processes=1, num_sanity_val_steps=2, num_workers=4, overfit_batches=0.0, plugins=None, precision=32, prepare_data_per_node=True, process_position=0, profiler=None, progress_bar_refresh_rate=None, reload_dataloaders_every_epoch=False, replace_sampler_ddp=True, resume_from_checkpoint=None, stochastic_weight_avg=False, sync_batchnorm=False, terminate_on_nan=False, test_file='data/test.tsv', tpu_cores=None, track_grad_norm=-1, train_file='data/train.tsv', truncated_bptt_steps=None, val_check_interval=1.0, warmup_ratio=0.1, weights_save_path=None, weights_summary='top')\n",
            "GPU available: True, used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "INFO:root:number of workers 4, data length 34242\n",
            "INFO:root:num_train_steps : 1712\n",
            "INFO:root:num_warmup_steps : 171\n",
            "\n",
            "  | Name  | Type                         | Params\n",
            "-------------------------------------------------------\n",
            "0 | model | BartForConditionalGeneration | 123 M \n",
            "-------------------------------------------------------\n",
            "123 M     Trainable params\n",
            "0         Non-trainable params\n",
            "123 M     Total params\n",
            "495.440   Total estimated model params size (MB)\n",
            "Epoch 0:  80% 3440/4282 [46:41<11:25,  1.23it/s, loss=1.32, v_num=0, val_loss=8.800, train_loss=1.290]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/857 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 0:  81% 3460/4282 [46:47<11:06,  1.23it/s, loss=1.32, v_num=0, val_loss=8.800, train_loss=1.290]\n",
            "Epoch 0:  81% 3480/4282 [46:52<10:48,  1.24it/s, loss=1.32, v_num=0, val_loss=8.800, train_loss=1.290]\n",
            "Epoch 0:  82% 3500/4282 [46:58<10:29,  1.24it/s, loss=1.32, v_num=0, val_loss=8.800, train_loss=1.290]\n",
            "Epoch 0:  82% 3520/4282 [47:04<10:11,  1.25it/s, loss=1.32, v_num=0, val_loss=8.800, train_loss=1.290]\n",
            "Epoch 0:  83% 3540/4282 [47:09<09:53,  1.25it/s, loss=1.32, v_num=0, val_loss=8.800, train_loss=1.290]\n",
            "Epoch 0:  83% 3560/4282 [47:15<09:35,  1.26it/s, loss=1.32, v_num=0, val_loss=8.800, train_loss=1.290]\n",
            "Epoch 0:  84% 3580/4282 [47:21<09:17,  1.26it/s, loss=1.32, v_num=0, val_loss=8.800, train_loss=1.290]\n",
            "Epoch 0:  84% 3600/4282 [47:26<08:59,  1.26it/s, loss=1.32, v_num=0, val_loss=8.800, train_loss=1.290]\n",
            "Epoch 0:  85% 3620/4282 [47:32<08:41,  1.27it/s, loss=1.32, v_num=0, val_loss=8.800, train_loss=1.290]\n",
            "Epoch 0:  85% 3640/4282 [47:37<08:24,  1.27it/s, loss=1.32, v_num=0, val_loss=8.800, train_loss=1.290]\n",
            "Epoch 0:  85% 3660/4282 [47:43<08:06,  1.28it/s, loss=1.32, v_num=0, val_loss=8.800, train_loss=1.290]\n",
            "Epoch 0:  86% 3680/4282 [47:49<07:49,  1.28it/s, loss=1.32, v_num=0, val_loss=8.800, train_loss=1.290]\n",
            "Epoch 0:  86% 3700/4282 [47:54<07:32,  1.29it/s, loss=1.32, v_num=0, val_loss=8.800, train_loss=1.290]\n",
            "Epoch 0:  87% 3720/4282 [48:00<07:15,  1.29it/s, loss=1.32, v_num=0, val_loss=8.800, train_loss=1.290]\n",
            "Epoch 0:  87% 3740/4282 [48:06<06:58,  1.30it/s, loss=1.32, v_num=0, val_loss=8.800, train_loss=1.290]\n",
            "Epoch 0:  88% 3760/4282 [48:11<06:41,  1.30it/s, loss=1.32, v_num=0, val_loss=8.800, train_loss=1.290]\n",
            "Epoch 0:  88% 3780/4282 [48:17<06:24,  1.30it/s, loss=1.32, v_num=0, val_loss=8.800, train_loss=1.290]\n",
            "Epoch 0:  89% 3800/4282 [48:23<06:08,  1.31it/s, loss=1.32, v_num=0, val_loss=8.800, train_loss=1.290]\n",
            "Epoch 0:  89% 3820/4282 [48:28<05:51,  1.31it/s, loss=1.32, v_num=0, val_loss=8.800, train_loss=1.290]\n",
            "Epoch 0:  90% 3840/4282 [48:34<05:35,  1.32it/s, loss=1.32, v_num=0, val_loss=8.800, train_loss=1.290]\n",
            "Epoch 0:  90% 3860/4282 [48:39<05:19,  1.32it/s, loss=1.32, v_num=0, val_loss=8.800, train_loss=1.290]\n",
            "Epoch 0:  91% 3880/4282 [48:45<05:03,  1.33it/s, loss=1.32, v_num=0, val_loss=8.800, train_loss=1.290]\n",
            "Epoch 0:  91% 3900/4282 [48:51<04:47,  1.33it/s, loss=1.32, v_num=0, val_loss=8.800, train_loss=1.290]\n",
            "Epoch 0:  92% 3920/4282 [48:56<04:31,  1.33it/s, loss=1.32, v_num=0, val_loss=8.800, train_loss=1.290]\n",
            "Epoch 0:  92% 3940/4282 [49:02<04:15,  1.34it/s, loss=1.32, v_num=0, val_loss=8.800, train_loss=1.290]\n",
            "Epoch 0:  92% 3960/4282 [49:08<03:59,  1.34it/s, loss=1.32, v_num=0, val_loss=8.800, train_loss=1.290]\n",
            "Epoch 0:  93% 3980/4282 [49:13<03:44,  1.35it/s, loss=1.32, v_num=0, val_loss=8.800, train_loss=1.290]\n",
            "Epoch 0:  93% 4000/4282 [49:19<03:28,  1.35it/s, loss=1.32, v_num=0, val_loss=8.800, train_loss=1.290]\n",
            "Epoch 0:  94% 4020/4282 [49:25<03:13,  1.36it/s, loss=1.32, v_num=0, val_loss=8.800, train_loss=1.290]\n",
            "Epoch 0:  94% 4040/4282 [49:30<02:57,  1.36it/s, loss=1.32, v_num=0, val_loss=8.800, train_loss=1.290]\n",
            "Epoch 0:  95% 4060/4282 [49:36<02:42,  1.36it/s, loss=1.32, v_num=0, val_loss=8.800, train_loss=1.290]\n",
            "Epoch 0:  95% 4080/4282 [49:42<02:27,  1.37it/s, loss=1.32, v_num=0, val_loss=8.800, train_loss=1.290]\n",
            "Epoch 0:  96% 4100/4282 [49:47<02:12,  1.37it/s, loss=1.32, v_num=0, val_loss=8.800, train_loss=1.290]\n",
            "Epoch 0:  96% 4120/4282 [49:53<01:57,  1.38it/s, loss=1.32, v_num=0, val_loss=8.800, train_loss=1.290]\n",
            "Epoch 0:  97% 4140/4282 [49:58<01:42,  1.38it/s, loss=1.32, v_num=0, val_loss=8.800, train_loss=1.290]\n",
            "Epoch 0:  97% 4160/4282 [50:04<01:28,  1.38it/s, loss=1.32, v_num=0, val_loss=8.800, train_loss=1.290]\n",
            "Epoch 0:  98% 4180/4282 [50:10<01:13,  1.39it/s, loss=1.32, v_num=0, val_loss=8.800, train_loss=1.290]\n",
            "Epoch 0:  98% 4200/4282 [50:15<00:58,  1.39it/s, loss=1.32, v_num=0, val_loss=8.800, train_loss=1.290]\n",
            "Epoch 0:  99% 4220/4282 [50:21<00:44,  1.40it/s, loss=1.32, v_num=0, val_loss=8.800, train_loss=1.290]\n",
            "Epoch 0:  99% 4240/4282 [50:27<00:29,  1.40it/s, loss=1.32, v_num=0, val_loss=8.800, train_loss=1.290]\n",
            "Epoch 0:  99% 4260/4282 [50:32<00:15,  1.40it/s, loss=1.32, v_num=0, val_loss=8.800, train_loss=1.290]\n",
            "Epoch 0: 100% 4280/4282 [50:38<00:01,  1.41it/s, loss=1.32, v_num=0, val_loss=8.800, train_loss=1.290]\n",
            "Epoch 0: 100% 4282/4282 [50:43<00:00,  1.41it/s, loss=1.32, v_num=0, val_loss=1.350, train_loss=1.230]\n",
            "                                                 \u001b[AEpoch 0, global step 3424: val_loss reached 1.34897 (best 1.34897), saving model to \"/content/drive/MyDrive/KoBART-summarization/logs/model_chp/epoch=00-val_loss=1.349.ckpt\" as top 1\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "tcmalloc: large alloc 1074479104 bytes == 0x5611df396000 @  0x7f7d442d8615 0x5611548c64cc 0x5611549a647a 0x5611548ccf0c 0x7f7d2a362934 0x7f7d2a364704 0x7f7d2a334430 0x7f7d1ac7a465 0x7f7d1ac769ca 0x7f7d1ac7b609 0x7f7d2a337dcb 0x7f7d29fbd0a0 0x5611548ca098 0x56115493d4d9 0x561154937ced 0x5611548cabda 0x561154938915 0x5611549379ee 0x5611548cabda 0x56115493cd00 0x5611548caafa 0x561154938915 0x5611548caafa 0x561154938c0d 0x5611548caafa 0x561154938c0d 0x5611549379ee 0x5611548cabda 0x561154938c0d 0x5611549379ee 0x5611548cabda\n",
            "tcmalloc: large alloc 1343102976 bytes == 0x56121f44a000 @  0x7f7d442d8615 0x5611548c64cc 0x5611549a647a 0x5611548ccf0c 0x7f7d2a362934 0x7f7d2a364704 0x7f7d2a334430 0x7f7d1ac7a465 0x7f7d1ac769ca 0x7f7d1ac7b609 0x7f7d2a337dcb 0x7f7d29fbd0a0 0x5611548ca098 0x56115493d4d9 0x561154937ced 0x5611548cabda 0x561154938915 0x5611549379ee 0x5611548cabda 0x56115493cd00 0x5611548caafa 0x561154938915 0x5611548caafa 0x561154938c0d 0x5611548caafa 0x561154938c0d 0x5611549379ee 0x5611548cabda 0x561154938c0d 0x5611549379ee 0x5611548cabda\n",
            "tcmalloc: large alloc 1678884864 bytes == 0x561282872000 @  0x7f7d442d8615 0x5611548c64cc 0x5611549a647a 0x5611548ccf0c 0x7f7d2a362934 0x7f7d2a364704 0x7f7d2a334430 0x7f7d1ac7a465 0x7f7d1ac769ca 0x7f7d1ac7b609 0x7f7d2a337dcb 0x7f7d29fbd0a0 0x5611548ca098 0x56115493d4d9 0x561154937ced 0x5611548cabda 0x561154938915 0x5611549379ee 0x5611548cabda 0x56115493cd00 0x5611548caafa 0x561154938915 0x5611548caafa 0x561154938c0d 0x5611548caafa 0x561154938c0d 0x5611549379ee 0x5611548cabda 0x561154938c0d 0x5611549379ee 0x5611548cabda\n",
            "Epoch 0: 100% 4282/4282 [50:49<00:00,  1.40it/s, loss=1.32, v_num=0, val_loss=1.350, train_loss=1.230]tcmalloc: large alloc 1678884864 bytes == 0x561282872000 @  0x7f7d442d8615 0x5611548c64cc 0x5611549a647a 0x5611548ccf0c 0x7f7d2a362934 0x7f7d2a364704 0x7f7d2a334430 0x7f7d1ac7a465 0x7f7d1ac769ca 0x7f7d1ac7b609 0x7f7d2a337dcb 0x7f7d29fbd0a0 0x5611548ca098 0x56115493d4d9 0x561154937ced 0x5611548cabda 0x561154938915 0x5611549379ee 0x5611548cabda 0x56115493cd00 0x5611548caafa 0x561154938915 0x5611548caafa 0x561154938c0d 0x5611548caafa 0x561154938c0d 0x5611549379ee 0x5611548cabda 0x561154938c0d 0x5611549379ee 0x5611548cabda\n",
            "Epoch 1:  80% 3440/4282 [46:42<11:26,  1.23it/s, loss=1.22, v_num=0, val_loss=1.350, train_loss=1.130]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/857 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 1:  81% 3460/4282 [46:48<11:07,  1.23it/s, loss=1.22, v_num=0, val_loss=1.350, train_loss=1.130]\n",
            "Epoch 1:  81% 3480/4282 [46:54<10:48,  1.24it/s, loss=1.22, v_num=0, val_loss=1.350, train_loss=1.130]\n",
            "Epoch 1:  82% 3500/4282 [46:59<10:30,  1.24it/s, loss=1.22, v_num=0, val_loss=1.350, train_loss=1.130]\n",
            "Epoch 1:  82% 3520/4282 [47:05<10:11,  1.25it/s, loss=1.22, v_num=0, val_loss=1.350, train_loss=1.130]\n",
            "Epoch 1:  83% 3540/4282 [47:11<09:53,  1.25it/s, loss=1.22, v_num=0, val_loss=1.350, train_loss=1.130]\n",
            "Epoch 1:  83% 3560/4282 [47:16<09:35,  1.25it/s, loss=1.22, v_num=0, val_loss=1.350, train_loss=1.130]\n",
            "Epoch 1:  84% 3580/4282 [47:22<09:17,  1.26it/s, loss=1.22, v_num=0, val_loss=1.350, train_loss=1.130]\n",
            "Epoch 1:  84% 3600/4282 [47:28<08:59,  1.26it/s, loss=1.22, v_num=0, val_loss=1.350, train_loss=1.130]\n",
            "Epoch 1:  85% 3620/4282 [47:33<08:41,  1.27it/s, loss=1.22, v_num=0, val_loss=1.350, train_loss=1.130]\n",
            "Epoch 1:  85% 3640/4282 [47:39<08:24,  1.27it/s, loss=1.22, v_num=0, val_loss=1.350, train_loss=1.130]\n",
            "Epoch 1:  85% 3660/4282 [47:45<08:06,  1.28it/s, loss=1.22, v_num=0, val_loss=1.350, train_loss=1.130]\n",
            "Epoch 1:  86% 3680/4282 [47:50<07:49,  1.28it/s, loss=1.22, v_num=0, val_loss=1.350, train_loss=1.130]\n",
            "Epoch 1:  86% 3700/4282 [47:56<07:32,  1.29it/s, loss=1.22, v_num=0, val_loss=1.350, train_loss=1.130]\n",
            "Epoch 1:  87% 3720/4282 [48:01<07:15,  1.29it/s, loss=1.22, v_num=0, val_loss=1.350, train_loss=1.130]\n",
            "Epoch 1:  87% 3740/4282 [48:07<06:58,  1.30it/s, loss=1.22, v_num=0, val_loss=1.350, train_loss=1.130]\n",
            "Epoch 1:  88% 3760/4282 [48:13<06:41,  1.30it/s, loss=1.22, v_num=0, val_loss=1.350, train_loss=1.130]\n",
            "Epoch 1:  88% 3780/4282 [48:18<06:24,  1.30it/s, loss=1.22, v_num=0, val_loss=1.350, train_loss=1.130]\n",
            "Epoch 1:  89% 3800/4282 [48:24<06:08,  1.31it/s, loss=1.22, v_num=0, val_loss=1.350, train_loss=1.130]\n",
            "Epoch 1:  89% 3820/4282 [48:30<05:51,  1.31it/s, loss=1.22, v_num=0, val_loss=1.350, train_loss=1.130]\n",
            "Epoch 1:  90% 3840/4282 [48:35<05:35,  1.32it/s, loss=1.22, v_num=0, val_loss=1.350, train_loss=1.130]\n",
            "Epoch 1:  90% 3860/4282 [48:41<05:19,  1.32it/s, loss=1.22, v_num=0, val_loss=1.350, train_loss=1.130]\n",
            "Epoch 1:  91% 3880/4282 [48:47<05:03,  1.33it/s, loss=1.22, v_num=0, val_loss=1.350, train_loss=1.130]\n",
            "Epoch 1:  91% 3900/4282 [48:52<04:47,  1.33it/s, loss=1.22, v_num=0, val_loss=1.350, train_loss=1.130]\n",
            "Epoch 1:  92% 3920/4282 [48:58<04:31,  1.33it/s, loss=1.22, v_num=0, val_loss=1.350, train_loss=1.130]\n",
            "Epoch 1:  92% 3940/4282 [49:04<04:15,  1.34it/s, loss=1.22, v_num=0, val_loss=1.350, train_loss=1.130]\n",
            "Epoch 1:  92% 3960/4282 [49:09<03:59,  1.34it/s, loss=1.22, v_num=0, val_loss=1.350, train_loss=1.130]\n",
            "Epoch 1:  93% 3980/4282 [49:15<03:44,  1.35it/s, loss=1.22, v_num=0, val_loss=1.350, train_loss=1.130]\n",
            "Epoch 1:  93% 4000/4282 [49:20<03:28,  1.35it/s, loss=1.22, v_num=0, val_loss=1.350, train_loss=1.130]\n",
            "Epoch 1:  94% 4020/4282 [49:26<03:13,  1.36it/s, loss=1.22, v_num=0, val_loss=1.350, train_loss=1.130]\n",
            "Epoch 1:  94% 4040/4282 [49:32<02:58,  1.36it/s, loss=1.22, v_num=0, val_loss=1.350, train_loss=1.130]\n",
            "Epoch 1:  95% 4060/4282 [49:37<02:42,  1.36it/s, loss=1.22, v_num=0, val_loss=1.350, train_loss=1.130]\n",
            "Epoch 1:  95% 4080/4282 [49:43<02:27,  1.37it/s, loss=1.22, v_num=0, val_loss=1.350, train_loss=1.130]\n",
            "Epoch 1:  96% 4100/4282 [49:49<02:12,  1.37it/s, loss=1.22, v_num=0, val_loss=1.350, train_loss=1.130]\n",
            "Epoch 1:  96% 4120/4282 [49:54<01:57,  1.38it/s, loss=1.22, v_num=0, val_loss=1.350, train_loss=1.130]\n",
            "Epoch 1:  97% 4140/4282 [50:00<01:42,  1.38it/s, loss=1.22, v_num=0, val_loss=1.350, train_loss=1.130]\n",
            "Epoch 1:  97% 4160/4282 [50:06<01:28,  1.38it/s, loss=1.22, v_num=0, val_loss=1.350, train_loss=1.130]\n",
            "Epoch 1:  98% 4180/4282 [50:11<01:13,  1.39it/s, loss=1.22, v_num=0, val_loss=1.350, train_loss=1.130]\n",
            "Epoch 1:  98% 4200/4282 [50:17<00:58,  1.39it/s, loss=1.22, v_num=0, val_loss=1.350, train_loss=1.130]\n",
            "Epoch 1:  99% 4220/4282 [50:23<00:44,  1.40it/s, loss=1.22, v_num=0, val_loss=1.350, train_loss=1.130]\n",
            "Epoch 1:  99% 4240/4282 [50:28<00:30,  1.40it/s, loss=1.22, v_num=0, val_loss=1.350, train_loss=1.130]\n",
            "Epoch 1:  99% 4260/4282 [50:34<00:15,  1.40it/s, loss=1.22, v_num=0, val_loss=1.350, train_loss=1.130]\n",
            "Epoch 1: 100% 4280/4282 [50:39<00:01,  1.41it/s, loss=1.22, v_num=0, val_loss=1.350, train_loss=1.130]\n",
            "Epoch 1: 100% 4282/4282 [50:44<00:00,  1.41it/s, loss=1.22, v_num=0, val_loss=1.330, train_loss=1.440]\n",
            "                                                 \u001b[AEpoch 1, global step 6849: val_loss reached 1.32875 (best 1.32875), saving model to \"/content/drive/MyDrive/KoBART-summarization/logs/model_chp/epoch=01-val_loss=1.329.ckpt\" as top 2\n",
            "Epoch 1: 100% 4282/4282 [50:50<00:00,  1.40it/s, loss=1.22, v_num=0, val_loss=1.330, train_loss=1.440]Saving latest checkpoint...\n",
            "Epoch 1: 100% 4282/4282 [51:09<00:00,  1.39it/s, loss=1.22, v_num=0, val_loss=1.330, train_loss=1.440]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N6JtaxjHyTNZ"
      },
      "source": [
        "# !python train.py  --gradient_clip_val 1.0 --max_epochs 3 --default_root_dir /content/drive/MyDrive/Colab_Notebooks/NLP/BART --gpus 1 \\\n",
        "# --batch_size 4 --num_workers 1 --checkpoint_path /content/drive/MyDrive/Colab_Notebooks/NLP/BART \\\n",
        "# --train_file /content/drive/MyDrive/Colab_Notebooks/NLP/BART/data/train.tsv \\\n",
        "# --test_file /content/drive/MyDrive/Colab_Notebooks/NLP/BART/data/test.tsv"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-eVmBvinXo3N",
        "outputId": "0dbb7bfe-1d61-4d23-e80b-353326a8ad77"
      },
      "source": [
        "!python get_model_binary.py --hparams ./logs/tb_logs/default/version_0/hparams.yaml --model_binary ./logs/model_chp/epoch=01-val_loss=1.329.ckpt"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "get_model_binary.py:13: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\n",
            "  hparams = yaml.load(f)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xk2HP5uZgly_"
      },
      "source": [
        "!pip install git+https://github.com/SKT-AI/KoBART#egg=kobart"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1LQbEnGRZoHu"
      },
      "source": [
        "# !streamlit run infer.py"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TWRoPSnjpclC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "be59dc4b-038d-419f-84a2-c4e6d77ef327"
      },
      "source": [
        "# test\n",
        "\n",
        "import torch\n",
        "from transformers import PreTrainedTokenizerFast\n",
        "from transformers.models.bart import BartForConditionalGeneration\n",
        "\n",
        "model = BartForConditionalGeneration.from_pretrained('./kobart_summary')\n",
        "tokenizer = PreTrainedTokenizerFast.from_pretrained('gogamza/kobart-base-v1')\n",
        "\n",
        "text = input()\n",
        "\n",
        "if text:\n",
        "    input_ids = tokenizer.encode(text)\n",
        "    input_ids = torch.tensor(input_ids)\n",
        "    input_ids = input_ids.unsqueeze(0)\n",
        "    output = model.generate(input_ids, eos_token_id=1, max_length=512, num_beams=5)\n",
        "    output = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "    print(output)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SK텔레콤은 우리말 인공지능(AI) 언어모델의 개발 역량 향상과 국어 정보화 저변 확대를 위해 ‘인공지능 언어 능력 평가 대회’를 개최한다고 1일 밝혔다. 이번 대회는 SK텔레콤·문화체육관광부·국립국어원 함께 주최한다. 대회는 9월 1일 오후 SK텔레콤의 기본 AI 언어모델과 국립국어원의 평가 데이터 세트 공개와 함께 시작되며, 참가자들은 오는 9월 15일부터 11월 1일까지 결과물을 수시로 업데이트하여 제출할 수 있다. 이번 대회는 개인 또는 팀으로 누구나 참가할 수 있다. 참가팀 중 대상(문화체육관광부 장관상), 금상·은상·동상, 특별상을 선정할 계획이며, 수상자 전원에게는 매년 SK텔레콤이 개최하는 AI 펠로우십(Fellowship)의 서류 심사 면제 특권도 제공할 계획이다. 참가자들은 제공된 언어 모델 등을 기반으로 각각의 AI 언어모델 프로그램을 개발하여 우리말을 이해하고 분석하는 능력을 평가받는다. 제출된 언어모델을 평가하는 과제는 4가지다. ▲문장의 문법 오류 판단하기(문장 적법성 판단) ▲맥락별 단어 의미 구별하기(동형이의어 구별) ▲문장 읽고 원인 추론하기(인과 관계 추론) ▲제시문 읽고 질문에 예/아니오 답하기(판정 의문문) 등이다. 이번 평가는 지금까지 한국어 인공지능 모델 평가를 위해 공개된 데이터세트들보다 난이도가 다소 높은 내용으로 구성됐다. SK텔레콤에서 제공하는 언어모델은 매개 변수가 12억개인 모델이다. 지난해 공개한 KoGPT2 모델보다 약 8배 크다. 이는 SK텔레콤이 국립국어원과 진행하고 있는 한국어 범용언어모델(GLM) 연구 과제의 초기 산출물로, 기존에 SK텔레콤이 개발해 발표한 KoBERT, KoGPT2, KoBART 모델에 이어 한국어 AI 모델을 개발·활용하고자 하는 이들에게 도움이 될 것으로 기대된다. 에릭 데이비스 SK텔레콤 Language Superintelligence Labs장은 “SK텔레콤과 국립국어원이 협업하여 준비한 이번 경진대회가 언어와 AI에 대한 역량을 맘껏 펼치는 장이 되길 기대한다”며 “나아가 이러한 건전한 경쟁이 범용언어모델을 비롯한 한국어 언어모델의 발전과 국어 정보화 확산에 기여하길 바란다”고 말했다.\n",
            "SK텔레콤은 우리말 인공지능(AI) 언어모델의 개발 역량 향상과 국어 정보화 저변 확대를 위해 ‘인공지능 언어 능력 평가 대회’를 개최한다고 1일 밝혔는데, 이번 대회는 9월 1일 오후 SK텔레콤의 기본 AI 언어모델과 국립국어원의 평가 데이터 세트 공개와 함께 시작되며, 참가자들은 오는 9월 15일부터 11월 1일까지 결과물을 수시로 업데이트하여 제출할 수 있다.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z5yxaX90e5F6"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}