{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install torch torchvision torchaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import PegasusForConditionalGeneration, PegasusTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_tokenizer = PegasusTokenizer.from_pretrained(\"google/pegasus-xsum\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import PreTrainedTokenizerFast, BartForConditionalGeneration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "ko_tokenizer = PreTrainedTokenizerFast.from_pretrained(\"ainize/kobart-news\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_model = PegasusForConditionalGeneration.from_pretrained(\"google/pegasus-xsum\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "ko_model = BartForConditionalGeneration.from_pretrained(\"ainize/kobart-news\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perform Abstractive Summarization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_article = \"\"\"Articles are found in many Indo-European languages, Semitic languages (only the definite article), and Polynesian languages; however, they are formally absent from many of the world's major languages including: Chinese, Japanese, Korean, Mongolian, many Turkic languages (incl. Tatar, Bashkir, Tuvan and Chuvash), many Uralic languages (incl. Finnic[a] and Saami languages), Indonesian, Hindi-Urdu, Punjabi, Tamil, the Baltic languages, the majority of Slavic languages, the Bantu languages (incl. Swahili) and Yoruba. In some languages that do have articles, such as some North Caucasian languages, the use of articles is optional; however, in others like English and German it is mandatory in all cases.\n",
    "\n",
    "Linguists believe the common ancestor of the Indo-European languages, Proto-Indo-European, did not have articles. Most of the languages in this family do not have definite or indefinite articles: there is no article in Latin or Sanskrit, nor in some modern Indo-European languages, such as the families of Slavic languages (except for Bulgarian and Macedonian, which are rather distinctive among the Slavic languages in their grammar, and some Northern Russian dialects[7]), Baltic languages and many Indo-Aryan languages. Although Classical Greek had a definite article (which has survived into Modern Greek and which bears strong functional resemblance to the German definite article, which it is related to), the earlier Homeric Greek used this article largely as a pronoun or demonstrative, whereas the earliest known form of Greek known as Mycenaean Greek did not have any articles. Articles developed independently in several language families.\n",
    "\n",
    "Not all languages have both definite and indefinite articles, and some languages have different types of definite and indefinite articles to distinguish finer shades of meaning: for example, French and Italian have a partitive article used for indefinite mass nouns, whereas Colognian has two distinct sets of definite articles indicating focus and uniqueness, and Macedonian uses definite articles in a demonstrative sense, with a tripartite distinction (proximal, medial, distal) based on distance from the speaker or interlocutor. The words this and that (and their plurals, these and those) can be understood in English as, ultimately, forms of the definite article the (whose declension in Old English included thaes, an ancestral form of this/that and these/those).\n",
    "\n",
    "In many languages, the form of the article may vary according to the gender, number, or case of its noun. In some languages the article may be the only indication of the case. Many languages do not use articles at all, and may use other ways of indicating old versus new information, such as topic–comment constructions.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = en_tokenizer(wiki_article, truncation=True, padding=\"longest\", return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[16643,   127,   374,   115,   223, 29504,   121, 17859,  4482,   108,\n",
       "           110, 42237,  4482,   143,  6026,   109, 13745,   974,   312,   111,\n",
       "         51585,  4482,   206,   802,   108,   157,   127, 11830, 12001,   135,\n",
       "           223,   113,   109,   278,   131,   116,   698,  4482,   330,   151,\n",
       "          1950,   108,  2769,   108,  5829,   108, 42911,   108,   223, 34664,\n",
       "          2288,  4482,   143, 62587,   107, 71491,   108, 25720, 35332,   108,\n",
       "         10374, 10313,   111, 15379,  2075, 15007,   312,   223, 75098,  2288,\n",
       "          4482,   143, 62587,   107, 18533,  2288,  4101,   304,  1100,   111,\n",
       "          8375, 13291,  4482,   312, 15828,   108, 12499,   121, 40549,  9423,\n",
       "           108, 32603,   108, 11635,   108,   109, 21201,  4482,   108,   109,\n",
       "          2198,   113, 66815,  4482,   108,   109,   596, 63661,  4482,   143,\n",
       "         62587,   107, 68119,   158,   111, 74321,   107,   222,   181,  4482,\n",
       "           120,   171,   133,  2391,   108,   253,   130,   181,   892, 47592,\n",
       "          4482,   108,   109,   207,   113,  2391,   117,  5754,   206,   802,\n",
       "           108,   115,   536,   172,  1188,   111,  2546,   126,   117,  7672,\n",
       "           115,   149,  1145,   107,   110, 53939,   116,   697,   109,   830,\n",
       "         27097,   113,   109, 29504,   121, 17859,  4482,   108, 39693,   121,\n",
       "           315,  2683,   121, 17859,   108,   368,   146,   133,  2391,   107,\n",
       "          1386,   113,   109,  4482,   115,   136,   328,   171,   146,   133,\n",
       "         13745,   132, 40140,  2391,   151,   186,   117,   220,   974,   115,\n",
       "          5249,   132, 31747,   108,  3001,   115,   181,   946, 29504,   121,\n",
       "         17859,  4482,   108,   253,   130,   109,  1252,   113, 66815,  4482,\n",
       "           143, 13344,   118, 28916,   111, 58752,   108,   162,   127,   880,\n",
       "          6328,   790,   109, 66815,  4482,   115,   153, 10947,   108,   111,\n",
       "           181,  3701,  3058, 50567,  4101,  1954, 38907,   108, 21201,  4482,\n",
       "           111,   223, 29504,   121,   251, 50110,  4482,   107,  2113, 19776,\n",
       "          4779,   196,   114, 13745,   974,   143,  2532,   148,  7646,   190,\n",
       "          4014,  4779,   111,   162,  9080,   806,  3819, 29836,   112,   109,\n",
       "          2546, 13745,   974,   108,   162,   126,   117,   985,   112,   312,\n",
       "           109,  1678,  1043, 11026,  4779,   263,   136,   974,  4318,   130,\n",
       "           114, 53166,   132, 85656,   108,  5990,   109,  9441,   606,   515,\n",
       "           113,  4779,   606,   130,   600, 56606, 19712,  4779,   368,   146,\n",
       "           133,   189,  2391,   107, 16643,  1184,  7539,   115,   500,  1261,\n",
       "          1252,   107,  1089,   149,  4482,   133,   302, 13745,   111, 40140,\n",
       "          2391,   108,   111,   181,  4482,   133,   291,  1020,   113, 13745,\n",
       "           111, 40140,  2391,   112, 12145, 22217,  6039,   113,  2050,   151,\n",
       "           118,   587,   108,  1775,   111,  2942,   133,   114,   297, 32242,\n",
       "           974,   263,   118, 40140,  2977, 46611,   108,  5990,  1398,  9096,\n",
       "         42359,   148,   228,  5057,  2120,   113, 13745,  2391, 10030,   777,\n",
       "           111, 20217,   108,   111, 58752,  1481, 13745,  2391,   115,   114,\n",
       "         85656,  1083,   108,   122,   114, 88216,  9797,   143, 48661, 47702,\n",
       "           108, 42752,   108, 46950,   158,   451,   124,  2028,   135,   109,\n",
       "          4041,   132, 75698,   107,   139,   989,   136,   111,   120,   143,\n",
       "           526,   153, 32410,   116,   108,   219,   111,   274,   158,   137,\n",
       "           129,  5045,   115,  1188,   130,   108,  3558,   108,  1878,   113,\n",
       "           109, 13745,   974,   109,   143, 45779, 27659, 43959,  2928,   115,\n",
       "          2960,  1188,   953, 40572,   772,   108,   142, 30361,   515,   113,\n",
       "           136,   191,  3713,   111,   219,   191, 19539,   250,   222,   223,\n",
       "          4482,   108,   109,   515,   113,   109,   974,   218,  3205,   992,\n",
       "           112,   109,  4336,   108,   344,   108,   132,   437,   113,   203,\n",
       "         25395,   107,   222,   181,  4482,   109,   974,   218,   129,   109,\n",
       "           209,     1]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[16643,   127,   374,   115,   223, 29504,   121, 17859,  4482,   108,\n",
       "            110, 42237,  4482,   143,  6026,   109, 13745,   974,   312,   111,\n",
       "          51585,  4482,   206,   802,   108,   157,   127, 11830, 12001,   135,\n",
       "            223,   113,   109,   278,   131,   116,   698,  4482,   330,   151,\n",
       "           1950,   108,  2769,   108,  5829,   108, 42911,   108,   223, 34664,\n",
       "           2288,  4482,   143, 62587,   107, 71491,   108, 25720, 35332,   108,\n",
       "          10374, 10313,   111, 15379,  2075, 15007,   312,   223, 75098,  2288,\n",
       "           4482,   143, 62587,   107, 18533,  2288,  4101,   304,  1100,   111,\n",
       "           8375, 13291,  4482,   312, 15828,   108, 12499,   121, 40549,  9423,\n",
       "            108, 32603,   108, 11635,   108,   109, 21201,  4482,   108,   109,\n",
       "           2198,   113, 66815,  4482,   108,   109,   596, 63661,  4482,   143,\n",
       "          62587,   107, 68119,   158,   111, 74321,   107,   222,   181,  4482,\n",
       "            120,   171,   133,  2391,   108,   253,   130,   181,   892, 47592,\n",
       "           4482,   108,   109,   207,   113,  2391,   117,  5754,   206,   802,\n",
       "            108,   115,   536,   172,  1188,   111,  2546,   126,   117,  7672,\n",
       "            115,   149,  1145,   107,   110, 53939,   116,   697,   109,   830,\n",
       "          27097,   113,   109, 29504,   121, 17859,  4482,   108, 39693,   121,\n",
       "            315,  2683,   121, 17859,   108,   368,   146,   133,  2391,   107,\n",
       "           1386,   113,   109,  4482,   115,   136,   328,   171,   146,   133,\n",
       "          13745,   132, 40140,  2391,   151,   186,   117,   220,   974,   115,\n",
       "           5249,   132, 31747,   108,  3001,   115,   181,   946, 29504,   121,\n",
       "          17859,  4482,   108,   253,   130,   109,  1252,   113, 66815,  4482,\n",
       "            143, 13344,   118, 28916,   111, 58752,   108,   162,   127,   880,\n",
       "           6328,   790,   109, 66815,  4482,   115,   153, 10947,   108,   111,\n",
       "            181,  3701,  3058, 50567,  4101,  1954, 38907,   108, 21201,  4482,\n",
       "            111,   223, 29504,   121,   251, 50110,  4482,   107,  2113, 19776,\n",
       "           4779,   196,   114, 13745,   974,   143,  2532,   148,  7646,   190,\n",
       "           4014,  4779,   111,   162,  9080,   806,  3819, 29836,   112,   109,\n",
       "           2546, 13745,   974,   108,   162,   126,   117,   985,   112,   312,\n",
       "            109,  1678,  1043, 11026,  4779,   263,   136,   974,  4318,   130,\n",
       "            114, 53166,   132, 85656,   108,  5990,   109,  9441,   606,   515,\n",
       "            113,  4779,   606,   130,   600, 56606, 19712,  4779,   368,   146,\n",
       "            133,   189,  2391,   107, 16643,  1184,  7539,   115,   500,  1261,\n",
       "           1252,   107,  1089,   149,  4482,   133,   302, 13745,   111, 40140,\n",
       "           2391,   108,   111,   181,  4482,   133,   291,  1020,   113, 13745,\n",
       "            111, 40140,  2391,   112, 12145, 22217,  6039,   113,  2050,   151,\n",
       "            118,   587,   108,  1775,   111,  2942,   133,   114,   297, 32242,\n",
       "            974,   263,   118, 40140,  2977, 46611,   108,  5990,  1398,  9096,\n",
       "          42359,   148,   228,  5057,  2120,   113, 13745,  2391, 10030,   777,\n",
       "            111, 20217,   108,   111, 58752,  1481, 13745,  2391,   115,   114,\n",
       "          85656,  1083,   108,   122,   114, 88216,  9797,   143, 48661, 47702,\n",
       "            108, 42752,   108, 46950,   158,   451,   124,  2028,   135,   109,\n",
       "           4041,   132, 75698,   107,   139,   989,   136,   111,   120,   143,\n",
       "            526,   153, 32410,   116,   108,   219,   111,   274,   158,   137,\n",
       "            129,  5045,   115,  1188,   130,   108,  3558,   108,  1878,   113,\n",
       "            109, 13745,   974,   109,   143, 45779, 27659, 43959,  2928,   115,\n",
       "           2960,  1188,   953, 40572,   772,   108,   142, 30361,   515,   113,\n",
       "            136,   191,  3713,   111,   219,   191, 19539,   250,   222,   223,\n",
       "           4482,   108,   109,   515,   113,   109,   974,   218,  3205,   992,\n",
       "            112,   109,  4336,   108,   344,   108,   132,   437,   113,   203,\n",
       "          25395,   107,   222,   181,  4482,   109,   974,   218,   129,   109,\n",
       "            209,     1]]),\n",
       " 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{**tokens}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/_tensor.py:575: UserWarning: floor_divide is deprecated, and will be removed in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values.\n",
      "To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:467.)\n",
      "  return torch.floor_divide(self, other)\n"
     ]
    }
   ],
   "source": [
    "summary = en_model.generate(**tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    0, 16643,   127,   374,   115,   223,  4482,   108,   155,   127,\n",
       "         11830, 12001,   135,   223,   113,   109,   278,   131,   116,   698,\n",
       "          4482,   107,     1]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Articles are found in many languages, but are formally absent from many of the world's major languages.\""
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_tokenizer.decode(summary[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "ko_article = \"\"\"자신이 원하는 것을 하려 하는 아이에게 ‘하지마’라는 말은 강한 부정적 감정을 불러일으킨다. 아이가 본인의 결정이 존중받지 못했다고 받아들일 수 있는 것. 이 기질의 아이들에게는 왜 하면 안 되는지에 대한 설명을 해줄 필요가 있다. 반면 자기주도적인 아이들은 자신이 하겠다고 한 말은 잘 지킨다. 예를 들어 아이가 태블릿을 갖고 놀고 싶어 한다면 “몇 시까지 할거야?”라고 묻고, 아이가 ‘30분’이라고 대답했다면 하게 두는 것도 좋다. 시간이 끝나기 전에 ‘10분 남았어’, ‘5분 남았어’라며 시간을 상기시켜주면 아이는 순순히 하던 것을 멈추고 부모에게 태블릿을 돌려줄 것이다.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = ko_tokenizer.encode(ko_article, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Summary Text Ids\n",
    "summary_text_ids = ko_model.generate(\n",
    "    input_ids=input_ids,\n",
    "    bos_token_id=ko_model.config.bos_token_id,\n",
    "    eos_token_id=ko_model.config.eos_token_id,\n",
    "    length_penalty=2.0,\n",
    "    max_length=142,\n",
    "    min_length=56,\n",
    "    num_beams=4,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "자 원하는 것을 하려 하는 아이에게 ‘하지마’라는 말은 강한 부정적 감정을 불러일으키는데 이 기질의 아이들에게는 왜 하면 안 되는지에 대한 설명을 해줄 필요가 있고 자기주도적인 아이들은 자신이 하겠다고 한 말은 잘 지킨다고 알려지며 시간이 끝나기 전에 시간이 끝나기 전에 ‘10분 남았어’, ‘5분 남았어’라며 시간을 상기시켜주면 아이는 순순히 하던 것을 멈추고 부모에게 태블릿을 돌려줄 것이다.\n"
     ]
    }
   ],
   "source": [
    "# Decoding Text\n",
    "print(ko_tokenizer.decode(summary_text_ids[0], skip_special_tokens=True))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "summarize",
   "language": "python",
   "name": "summarize"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
